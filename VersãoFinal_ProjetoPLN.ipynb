{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseArthurSoares/ProjetoFinal-PLN/blob/main/Vers%C3%A3oFinal_ProjetoPLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identificação dos alunos**\n",
        "\n",
        "**Email: alexandre.santos@ccc.ufcg.edu.br**\n",
        "\n",
        "**Email: jose.arthur.bezerra@ccc.ufcg.edu.br**\n",
        "\n",
        "**Matrícula: 119210883**\n",
        "\n",
        "**Matrícula: 121110566**"
      ],
      "metadata": {
        "id": "2hom1ypIlvQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construindo o RAG System"
      ],
      "metadata": {
        "id": "NtusTriTl1AK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problema**\n",
        "\n",
        "A dificuldade em encontrar artigos relevantes, seja por métodos manuais ou por meio de recomendações de modelos de linguagem de última geração (LLMs) como ChatGPT ou Gemini, é um problema significativo no contexto acadêmico e de pesquisa."
      ],
      "metadata": {
        "id": "qcxr2V07l1v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo**\n",
        "\n",
        "O objetivo deste projeto é criar um sistema inteligente de recomendação de papers acadêmicos que utiliza técnicas de recuperação da informação e processamento de linguagem natural (NLP) para encontrar e sugerir artigos relevantes com base nas consultas dos usuários."
      ],
      "metadata": {
        "id": "6k5mGiqWl30W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O que usamos?**\n",
        "\n",
        "\n",
        "*   RAG System (Retrieval-Augmented Generation) -  é uma abordagem que combina a recuperação de informações com a geração de texto. Em vez de depender exclusivamente de um modelo de linguagem para gerar respostas, o sistema primeiro recupera informações relevantes de uma base de dados ou de documentos e, em seguida, usa essas informações como contexto para gerar uma resposta mais precisa e informativa.\n",
        "*   Gemini - É uma série de modelos de inteligência artificial desenvolvidos pelo Google DeepMind, focados em tarefas de aprendizado profundo, incluindo geração de texto, compreensão de linguagem natural e outras aplicações relacionadas à IA.\n",
        "*   MongoDB - MongoDB é um banco de dados NoSQL orientado a documentos que permite o armazenamento e a recuperação de dados de forma flexível e escalável. Ele armazena dados em formato JSON (ou BSON), o que permite que os desenvolvedores trabalhem com dados não estruturados ou semiestruturados de maneira mais natural.\n"
      ],
      "metadata": {
        "id": "yN_dNedUl_tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 1: Instalando as bibliotecas\n"
      ],
      "metadata": {
        "id": "ZOx6wRe_mCsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas pymongo sentence_transformers\n",
        "!pip install -U transformers\n",
        "# Install below if using GPU\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "RmoG9Nr9mI0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Recuperando e preparando os dados"
      ],
      "metadata": {
        "id": "V9BGTWe8mOeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos a API arXiv que é O arXiv é um repositório online de pré-publicações científicas e artigos acadêmicos, que permite a pesquisadores de diversas áreas compartilhar suas pesquisas antes da revisão por pares. Criado em 1991, o arXiv é amplamente utilizado nas disciplinas de física, matemática, ciência da computação, biologia quantitativa, estatística e mais."
      ],
      "metadata": {
        "id": "lxI73F54mRZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "\n",
        "# Function to get data from arXiv API\n",
        "def get_data(query, max_results, max_retries):\n",
        "    url = f\"http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            root = ElementTree.fromstring(response.content)\n",
        "            xml_namespace = \"{http://www.w3.org/2005/Atom}\"\n",
        "            papers = [{'title': entry.find(f'{xml_namespace}title').text,\n",
        "                       'summary': entry.find(f'{xml_namespace}summary').text,\n",
        "                       'link': entry.find(f'{xml_namespace}id').text}\n",
        "                      for entry in root.findall(f'{xml_namespace}entry')]\n",
        "            return papers\n",
        "        except requests.ConnectionError:\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "query = \"Computer science OR software engineer OR artificial intelligence\"\n",
        "papers = get_data(query, max_results=1000, max_retries=5)\n",
        "for paper in papers[0:10]:\n",
        "    print(paper)"
      ],
      "metadata": {
        "id": "qvkOagAwmT_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré-processamento dos dados com o objetivo de melhorar a qualidade dos dados.  Foi utilizado as seguintes técnicas:\n",
        "\n",
        "* Tokenização: Divide o texto em palavras e as transforma para minúsculas.\n",
        "* Remoção de Stop Words: Remove palavras comuns e pontuações que não agregam valor à análise.\n",
        "* Filtragem de Comprimento: Mantém apenas tokens com comprimento entre 3 e 15 caracteres.\n",
        "* Lematização: Reduz palavras à sua forma base, como transformar \"caminhando\" em \"caminhar\"."
      ],
      "metadata": {
        "id": "HxCZA-VKmhdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if 3 <= len(token) <= 15]\n",
        "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(processed_tokens)  # Retorna como uma string separada por espaços\n",
        "\n",
        "\n",
        "for paper in papers:\n",
        "    paper['summary'] = preprocess_text(paper['summary'])"
      ],
      "metadata": {
        "id": "YDbxE8LKmrqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foi criado um DataFrame a partir de papers, selecionando apenas as colunas relevantes (título, resumo e link). Isso simplifica a análise, focando apenas nas informações importantes."
      ],
      "metadata": {
        "id": "xwyT_QNzmuN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_df = pd.DataFrame(papers)\n",
        "relevant_columns = ['title', 'summary', 'link']\n",
        "dataset_df = dataset_df[relevant_columns]\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "M7zdFpdSmwfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concantenando todas as colunas relevantes para gerar um embedding:"
      ],
      "metadata": {
        "id": "2njap4Hrm4Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df['text_to_embed'] = (\n",
        "    dataset_df['title'].astype(str) + ' ' +\n",
        "    dataset_df['summary'].astype(str) + ' ' + \\\n",
        "    dataset_df['link'].astype(str) + ' '\n",
        ")\n",
        "\n",
        "dataset_df['text_to_embed'][0]"
      ],
      "metadata": {
        "id": "PU15FL64m8RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3: Gerando os embeddings"
      ],
      "metadata": {
        "id": "_x0R-lkznBz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modelo: Inicializa o modelo gte-large para embeddings.\n",
        "\n",
        "* Função: generate_embeddings retorna uma lista vazia para textos vazios ou gera embeddings para textos não vazios.\n",
        "* Aplicação: Aplica a função à coluna text_to_embed do DataFrame dataset_df, adicionando uma nova coluna embedding com os resultados."
      ],
      "metadata": {
        "id": "qK5uZfmznH22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer('thenlper/gte-large')\n",
        "\n",
        "\n",
        "def generate_embeddings(text):\n",
        "  if not text.strip():\n",
        "    # retorna uma lista fazia caso o texto passado seja vazio\n",
        "    print(\"Attempted to get embedding for empty text\")\n",
        "    return []\n",
        "\n",
        "  embedding = embedding_model.encode(text)\n",
        "  return embedding.tolist()\n",
        "\n",
        "dataset_df['embedding'] = dataset_df['text_to_embed'].apply(generate_embeddings)\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "4bPBLtcznKY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df['embedding'][:5]"
      ],
      "metadata": {
        "id": "4HEW7HOynOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 4: Estabelecendo conexão de dados"
      ],
      "metadata": {
        "id": "oONZ7k37nQ0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estabelecendo uma conexão com um banco de dados MongoDB:"
      ],
      "metadata": {
        "id": "NtuGIGSKnUIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "#from google.colab import userdata\n",
        "\n",
        "\n",
        "def get_mongo_client(mongo_uri):\n",
        "  \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "  try:\n",
        "    client = pymongo.MongoClient(mongo_uri)\n",
        "    print(\"Connection to MongoDB successful\")\n",
        "    return client\n",
        "  except pymongo.errors.ConnectionFailure as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "    return None\n",
        "\n",
        "mongo_uri = \"mongodb+srv://projetopln:projeto1234@projetopln.jaqd1.mongodb.net/?retryWrites=true&w=majority&appName=ProjetoPLN\"\n",
        "mongo_client = get_mongo_client(mongo_uri)\n",
        "\n",
        "# Ingest data into MongoDB\n",
        "db = mongo_client[\"games\"]\n",
        "collection = db[\"games_collection\"]"
      ],
      "metadata": {
        "id": "aeKMwpNRncJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "IE7SfYGinjAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = dataset_df.to_dict(\"records\")\n",
        "collection.insert_many(documents)\n",
        "\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ],
      "metadata": {
        "id": "p2mEKkhunmhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = collection.find()\n",
        "for doc in docs:\n",
        "  print(doc)\n",
        "  break"
      ],
      "metadata": {
        "id": "U7THOIRknr4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 5: Executar busca vetorial nas consultas do usuário"
      ],
      "metadata": {
        "id": "6kzB6jsHnxLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Geração de Embeddings: Gera embeddings para a consulta do usuário usando a função generate_embeddings.\n",
        "\n",
        "* Pipeline de Busca Vetorial:\n",
        "\n",
        "    * $vectorSearch: Realiza uma busca vetorial na coleção com base no embedding gerado, considerando até 150 candidatos e retornando os 4 melhores resultados.\n",
        "    * $project: Define os campos a serem retornados, excluindo o _id e incluindo title, summary, link e a pontuação da busca.\n",
        "\n",
        "* Execução da Busca: Executa a pipeline de agregação na coleção e retorna os resultados como uma lista."
      ],
      "metadata": {
        "id": "WLazoBSvn0Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(user_query, collection):\n",
        "    query_embedding = generate_embeddings(user_query)\n",
        "\n",
        "    if query_embedding is None:\n",
        "        return \"Invalid query or embedding generation failed.\"\n",
        "\n",
        "    # Define the vector search pipeline\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
        "                \"limit\": 4,  # Return top 4 matches\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,  # Exclude the _id field\n",
        "                \"title\": 1,  # Include the title field\n",
        "                \"summary\": 1,  # Include the summary field\n",
        "                \"link\": 1,  # Include the link field\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
        "            }\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Execute the search\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)"
      ],
      "metadata": {
        "id": "AuWZHYAun3s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Busca por Resultados: Chama a função vector_search com a consulta do usuário para obter artigos relevantes da coleção.\n",
        "\n",
        "* Formatação dos Resultados: Para cada resultado encontrado, formata o título, resumo e link em uma string, assegurando que cada informação esteja claramente apresentada."
      ],
      "metadata": {
        "id": "C3fUqQ7-n9Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_result(query, collection):\n",
        "    get_knowledge = vector_search(query, collection)\n",
        "\n",
        "    if not get_knowledge:\n",
        "        return \"No relevant articles found.\"\n",
        "\n",
        "    search_result = \"\"\n",
        "    for result in get_knowledge:\n",
        "        search_result += f\"Title: {result.get('title', 'N/A')}\\n\"\n",
        "        search_result += f\"Summary: {result.get('summary', 'N/A')}\\n\"\n",
        "        search_result += f\"Link: {result.get('link', 'N/A')}\\n\\n\"  # Ensure link is included\n",
        "\n",
        "    return search_result\n"
      ],
      "metadata": {
        "id": "ET630dHtn_VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 6: Definição do Prompt e do Modelo de Linguagem (LLM)"
      ],
      "metadata": {
        "id": "29_g2rlYoCfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gera um prompt para recomendar artigos. O prompt inclui instruções para recomendar apenas artigos relevantes. Além disso, Define um formato específico para a saída, incluindo título do artigo, resumo, link e uma justificativa para a recomendação.\n",
        "\n"
      ],
      "metadata": {
        "id": "2LuLxTs-oD5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(query, context):\n",
        "    prompt = (\n",
        "        \"Recommend academic articles to the user based on their query and the following articles retrieved from the database. \"\n",
        "        \"Only recommend articles if they are relevant to the query. If none of the retrieved articles are relevant, inform the user that \"\n",
        "        \"you couldn't find any relevant articles.\\n\\n\"\n",
        "        \"# OUTPUT TEMPLATE\\n\"\n",
        "        \"- **Article Title**:\\n\"\n",
        "        \"    - Abstract:\\n\"\n",
        "        \"    - Link:\\n\"\n",
        "        \"    - (Your justification for recommending this article)\\n\\n\"\n",
        "        f\"Query: {query}\\nContext:\\n{context}\"\n",
        "    )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "IrqsacrToJ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "cMd1nEc-oNd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora é necessário carregar o tokenizador e o modelo de linguagem \"gemma-2b-it\" da Google usando a biblioteca Hugging Face Transformers. Ele configura o modelo para gerar texto, automaticamente utilizando GPU ou CPU conforme disponível."
      ],
      "metadata": {
        "id": "A5iL5z-WoP6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "# CPU Enabled uncomment below 👇🏽\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "# GPU Enabled use below 👇🏽\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "4WBEfvjdoUoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 7: Testes"
      ],
      "metadata": {
        "id": "0G24SJbpoZnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando uma consulta mais geral: \"Artigos sobre llms\""
      ],
      "metadata": {
        "id": "fqITmJ0yohtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"papers about llms\"\n",
        "source_information = get_search_result(query, collection)\n",
        "prompt = generate_prompt(query, source_information)\n",
        "\n",
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,  # Usa amostragem para aumentar a diversidade\n",
        "    temperature=0.7,  # Controla a aleatoriedade\n",
        "    top_p=0.9,        # Nucleus sampling para limitar as probabilidades cumulativas\n",
        "    repetition_penalty=1.2  # Penaliza repetições\n",
        ")\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "49i_F4Hzoj4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando uma consulta mais específica: \"Artigos sobre inteligência artifical na manufatura aditiva\""
      ],
      "metadata": {
        "id": "FfmlDA5uopjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"papers about artificial intelligence in additive manufacturing\"\n",
        "source_information = get_search_result(query, collection)\n",
        "prompt = generate_prompt(query, source_information)\n",
        "\n",
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,  # Usa amostragem para aumentar a diversidade\n",
        "    temperature=0.7,  # Controla a aleatoriedade\n",
        "    top_p=0.9,        # Nucleus sampling para limitar as probabilidades cumulativas\n",
        "    repetition_penalty=1.2  # Penaliza repetições\n",
        ")\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "hUcWXuTQoqxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPh36M3iEtblezeXOcRQd+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}