{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoseArthurSoares/ProjetoFinal-PLN/blob/main/Vers%C3%A3oFinal_ProjetoPLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifica√ß√£o dos alunos**\n",
        "\n",
        "**Email: alexandre.santos@ccc.ufcg.edu.br**\n",
        "\n",
        "**Email: jose.arthur.bezerra@ccc.ufcg.edu.br**\n",
        "\n",
        "**Matr√≠cula: 119210883**\n",
        "\n",
        "**Matr√≠cula: 121110566**"
      ],
      "metadata": {
        "id": "2hom1ypIlvQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construindo o RAG System"
      ],
      "metadata": {
        "id": "NtusTriTl1AK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problema**\n",
        "\n",
        "A dificuldade em encontrar artigos relevantes, seja por m√©todos manuais ou por meio de recomenda√ß√µes de modelos de linguagem de √∫ltima gera√ß√£o (LLMs) como ChatGPT ou Gemini, √© um problema significativo no contexto acad√™mico e de pesquisa."
      ],
      "metadata": {
        "id": "qcxr2V07l1v_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo**\n",
        "\n",
        "O objetivo deste projeto √© criar um sistema inteligente de recomenda√ß√£o de papers acad√™micos que utiliza t√©cnicas de recupera√ß√£o da informa√ß√£o e processamento de linguagem natural (NLP) para encontrar e sugerir artigos relevantes com base nas consultas dos usu√°rios."
      ],
      "metadata": {
        "id": "6k5mGiqWl30W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O que usamos?**\n",
        "\n",
        "\n",
        "*   RAG System (Retrieval-Augmented Generation) -  √© uma abordagem que combina a recupera√ß√£o de informa√ß√µes com a gera√ß√£o de texto. Em vez de depender exclusivamente de um modelo de linguagem para gerar respostas, o sistema primeiro recupera informa√ß√µes relevantes de uma base de dados ou de documentos e, em seguida, usa essas informa√ß√µes como contexto para gerar uma resposta mais precisa e informativa.\n",
        "*   Gemini - √â uma s√©rie de modelos de intelig√™ncia artificial desenvolvidos pelo Google DeepMind, focados em tarefas de aprendizado profundo, incluindo gera√ß√£o de texto, compreens√£o de linguagem natural e outras aplica√ß√µes relacionadas √† IA.\n",
        "*   MongoDB - MongoDB √© um banco de dados NoSQL orientado a documentos que permite o armazenamento e a recupera√ß√£o de dados de forma flex√≠vel e escal√°vel. Ele armazena dados em formato JSON (ou BSON), o que permite que os desenvolvedores trabalhem com dados n√£o estruturados ou semiestruturados de maneira mais natural.\n"
      ],
      "metadata": {
        "id": "yN_dNedUl_tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 1: Instalando as bibliotecas\n"
      ],
      "metadata": {
        "id": "ZOx6wRe_mCsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pandas pymongo sentence_transformers\n",
        "!pip install -U transformers\n",
        "# Install below if using GPU\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "RmoG9Nr9mI0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Recuperando e preparando os dados"
      ],
      "metadata": {
        "id": "V9BGTWe8mOeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos a API arXiv que √© O arXiv √© um reposit√≥rio online de pr√©-publica√ß√µes cient√≠ficas e artigos acad√™micos, que permite a pesquisadores de diversas √°reas compartilhar suas pesquisas antes da revis√£o por pares. Criado em 1991, o arXiv √© amplamente utilizado nas disciplinas de f√≠sica, matem√°tica, ci√™ncia da computa√ß√£o, biologia quantitativa, estat√≠stica e mais."
      ],
      "metadata": {
        "id": "lxI73F54mRZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "\n",
        "# Function to get data from arXiv API\n",
        "def get_data(query, max_results, max_retries):\n",
        "    url = f\"http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={max_results}\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            root = ElementTree.fromstring(response.content)\n",
        "            xml_namespace = \"{http://www.w3.org/2005/Atom}\"\n",
        "            papers = [{'title': entry.find(f'{xml_namespace}title').text,\n",
        "                       'summary': entry.find(f'{xml_namespace}summary').text,\n",
        "                       'link': entry.find(f'{xml_namespace}id').text}\n",
        "                      for entry in root.findall(f'{xml_namespace}entry')]\n",
        "            return papers\n",
        "        except requests.ConnectionError:\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "query = \"Computer science OR software engineer OR artificial intelligence\"\n",
        "papers = get_data(query, max_results=1000, max_retries=5)\n",
        "for paper in papers[0:10]:\n",
        "    print(paper)"
      ],
      "metadata": {
        "id": "qvkOagAwmT_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pr√©-processamento dos dados com o objetivo de melhorar a qualidade dos dados.  Foi utilizado as seguintes t√©cnicas:\n",
        "\n",
        "* Tokeniza√ß√£o: Divide o texto em palavras e as transforma para min√∫sculas.\n",
        "* Remo√ß√£o de Stop Words: Remove palavras comuns e pontua√ß√µes que n√£o agregam valor √† an√°lise.\n",
        "* Filtragem de Comprimento: Mant√©m apenas tokens com comprimento entre 3 e 15 caracteres.\n",
        "* Lematiza√ß√£o: Reduz palavras √† sua forma base, como transformar \"caminhando\" em \"caminhar\"."
      ],
      "metadata": {
        "id": "HxCZA-VKmhdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if 3 <= len(token) <= 15]\n",
        "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(processed_tokens)  # Retorna como uma string separada por espa√ßos\n",
        "\n",
        "\n",
        "for paper in papers:\n",
        "    paper['summary'] = preprocess_text(paper['summary'])"
      ],
      "metadata": {
        "id": "YDbxE8LKmrqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foi criado um DataFrame a partir de papers, selecionando apenas as colunas relevantes (t√≠tulo, resumo e link). Isso simplifica a an√°lise, focando apenas nas informa√ß√µes importantes."
      ],
      "metadata": {
        "id": "xwyT_QNzmuN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_df = pd.DataFrame(papers)\n",
        "relevant_columns = ['title', 'summary', 'link']\n",
        "dataset_df = dataset_df[relevant_columns]\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "M7zdFpdSmwfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concantenando todas as colunas relevantes para gerar um embedding:"
      ],
      "metadata": {
        "id": "2njap4Hrm4Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df['text_to_embed'] = (\n",
        "    dataset_df['title'].astype(str) + ' ' +\n",
        "    dataset_df['summary'].astype(str) + ' ' + \\\n",
        "    dataset_df['link'].astype(str) + ' '\n",
        ")\n",
        "\n",
        "dataset_df['text_to_embed'][0]"
      ],
      "metadata": {
        "id": "PU15FL64m8RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3: Gerando os embeddings"
      ],
      "metadata": {
        "id": "_x0R-lkznBz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modelo: Inicializa o modelo gte-large para embeddings.\n",
        "\n",
        "* Fun√ß√£o: generate_embeddings retorna uma lista vazia para textos vazios ou gera embeddings para textos n√£o vazios.\n",
        "* Aplica√ß√£o: Aplica a fun√ß√£o √† coluna text_to_embed do DataFrame dataset_df, adicionando uma nova coluna embedding com os resultados."
      ],
      "metadata": {
        "id": "qK5uZfmznH22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer('thenlper/gte-large')\n",
        "\n",
        "\n",
        "def generate_embeddings(text):\n",
        "  if not text.strip():\n",
        "    # retorna uma lista fazia caso o texto passado seja vazio\n",
        "    print(\"Attempted to get embedding for empty text\")\n",
        "    return []\n",
        "\n",
        "  embedding = embedding_model.encode(text)\n",
        "  return embedding.tolist()\n",
        "\n",
        "dataset_df['embedding'] = dataset_df['text_to_embed'].apply(generate_embeddings)\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "4bPBLtcznKY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df['embedding'][:5]"
      ],
      "metadata": {
        "id": "4HEW7HOynOEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 4: Estabelecendo conex√£o de dados"
      ],
      "metadata": {
        "id": "oONZ7k37nQ0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estabelecendo uma conex√£o com um banco de dados MongoDB:"
      ],
      "metadata": {
        "id": "NtuGIGSKnUIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "#from google.colab import userdata\n",
        "\n",
        "\n",
        "def get_mongo_client(mongo_uri):\n",
        "  \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "  try:\n",
        "    client = pymongo.MongoClient(mongo_uri)\n",
        "    print(\"Connection to MongoDB successful\")\n",
        "    return client\n",
        "  except pymongo.errors.ConnectionFailure as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "    return None\n",
        "\n",
        "mongo_uri = \"mongodb+srv://projetopln:projeto1234@projetopln.jaqd1.mongodb.net/?retryWrites=true&w=majority&appName=ProjetoPLN\"\n",
        "mongo_client = get_mongo_client(mongo_uri)\n",
        "\n",
        "# Ingest data into MongoDB\n",
        "db = mongo_client[\"games\"]\n",
        "collection = db[\"games_collection\"]"
      ],
      "metadata": {
        "id": "aeKMwpNRncJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "IE7SfYGinjAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = dataset_df.to_dict(\"records\")\n",
        "collection.insert_many(documents)\n",
        "\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ],
      "metadata": {
        "id": "p2mEKkhunmhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = collection.find()\n",
        "for doc in docs:\n",
        "  print(doc)\n",
        "  break"
      ],
      "metadata": {
        "id": "U7THOIRknr4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 5: Executar busca vetorial nas consultas do usu√°rio"
      ],
      "metadata": {
        "id": "6kzB6jsHnxLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Gera√ß√£o de Embeddings: Gera embeddings para a consulta do usu√°rio usando a fun√ß√£o generate_embeddings.\n",
        "\n",
        "* Pipeline de Busca Vetorial:\n",
        "\n",
        "    * $vectorSearch: Realiza uma busca vetorial na cole√ß√£o com base no embedding gerado, considerando at√© 150 candidatos e retornando os 4 melhores resultados.\n",
        "    * $project: Define os campos a serem retornados, excluindo o _id e incluindo title, summary, link e a pontua√ß√£o da busca.\n",
        "\n",
        "* Execu√ß√£o da Busca: Executa a pipeline de agrega√ß√£o na cole√ß√£o e retorna os resultados como uma lista."
      ],
      "metadata": {
        "id": "WLazoBSvn0Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_search(user_query, collection):\n",
        "    query_embedding = generate_embeddings(user_query)\n",
        "\n",
        "    if query_embedding is None:\n",
        "        return \"Invalid query or embedding generation failed.\"\n",
        "\n",
        "    # Define the vector search pipeline\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"path\": \"embedding\",\n",
        "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
        "                \"limit\": 4,  # Return top 4 matches\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,  # Exclude the _id field\n",
        "                \"title\": 1,  # Include the title field\n",
        "                \"summary\": 1,  # Include the summary field\n",
        "                \"link\": 1,  # Include the link field\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
        "            }\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Execute the search\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)"
      ],
      "metadata": {
        "id": "AuWZHYAun3s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Busca por Resultados: Chama a fun√ß√£o vector_search com a consulta do usu√°rio para obter artigos relevantes da cole√ß√£o.\n",
        "\n",
        "* Formata√ß√£o dos Resultados: Para cada resultado encontrado, formata o t√≠tulo, resumo e link em uma string, assegurando que cada informa√ß√£o esteja claramente apresentada."
      ],
      "metadata": {
        "id": "C3fUqQ7-n9Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search_result(query, collection):\n",
        "    get_knowledge = vector_search(query, collection)\n",
        "\n",
        "    if not get_knowledge:\n",
        "        return \"No relevant articles found.\"\n",
        "\n",
        "    search_result = \"\"\n",
        "    for result in get_knowledge:\n",
        "        search_result += f\"Title: {result.get('title', 'N/A')}\\n\"\n",
        "        search_result += f\"Summary: {result.get('summary', 'N/A')}\\n\"\n",
        "        search_result += f\"Link: {result.get('link', 'N/A')}\\n\\n\"  # Ensure link is included\n",
        "\n",
        "    return search_result\n"
      ],
      "metadata": {
        "id": "ET630dHtn_VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 6: Defini√ß√£o do Prompt e do Modelo de Linguagem (LLM)"
      ],
      "metadata": {
        "id": "29_g2rlYoCfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gera um prompt para recomendar artigos. O prompt inclui instru√ß√µes para recomendar apenas artigos relevantes. Al√©m disso, Define um formato espec√≠fico para a sa√≠da, incluindo t√≠tulo do artigo, resumo, link e uma justificativa para a recomenda√ß√£o.\n",
        "\n"
      ],
      "metadata": {
        "id": "2LuLxTs-oD5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(query, context):\n",
        "    prompt = (\n",
        "        \"Recommend academic articles to the user based on their query and the following articles retrieved from the database. \"\n",
        "        \"Only recommend articles if they are relevant to the query. If none of the retrieved articles are relevant, inform the user that \"\n",
        "        \"you couldn't find any relevant articles.\\n\\n\"\n",
        "        \"# OUTPUT TEMPLATE\\n\"\n",
        "        \"- **Article Title**:\\n\"\n",
        "        \"    - Abstract:\\n\"\n",
        "        \"    - Link:\\n\"\n",
        "        \"    - (Your justification for recommending this article)\\n\\n\"\n",
        "        f\"Query: {query}\\nContext:\\n{context}\"\n",
        "    )\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "IrqsacrToJ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "cMd1nEc-oNd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora √© necess√°rio carregar o tokenizador e o modelo de linguagem \"gemma-2b-it\" da Google usando a biblioteca Hugging Face Transformers. Ele configura o modelo para gerar texto, automaticamente utilizando GPU ou CPU conforme dispon√≠vel."
      ],
      "metadata": {
        "id": "A5iL5z-WoP6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "# CPU Enabled uncomment below üëáüèΩ\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "# GPU Enabled use below üëáüèΩ\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "4WBEfvjdoUoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 7: Testes"
      ],
      "metadata": {
        "id": "0G24SJbpoZnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando uma consulta mais geral: \"Artigos sobre llms\""
      ],
      "metadata": {
        "id": "fqITmJ0yohtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"papers about llms\"\n",
        "source_information = get_search_result(query, collection)\n",
        "prompt = generate_prompt(query, source_information)\n",
        "\n",
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,  # Usa amostragem para aumentar a diversidade\n",
        "    temperature=0.7,  # Controla a aleatoriedade\n",
        "    top_p=0.9,        # Nucleus sampling para limitar as probabilidades cumulativas\n",
        "    repetition_penalty=1.2  # Penaliza repeti√ß√µes\n",
        ")\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "49i_F4Hzoj4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando uma consulta mais espec√≠fica: \"Artigos sobre intelig√™ncia artifical na manufatura aditiva\""
      ],
      "metadata": {
        "id": "FfmlDA5uopjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"papers about artificial intelligence in additive manufacturing\"\n",
        "source_information = get_search_result(query, collection)\n",
        "prompt = generate_prompt(query, source_information)\n",
        "\n",
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,  # Usa amostragem para aumentar a diversidade\n",
        "    temperature=0.7,  # Controla a aleatoriedade\n",
        "    top_p=0.9,        # Nucleus sampling para limitar as probabilidades cumulativas\n",
        "    repetition_penalty=1.2  # Penaliza repeti√ß√µes\n",
        ")\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "hUcWXuTQoqxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPh36M3iEtblezeXOcRQd+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}